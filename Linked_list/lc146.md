# 146. LRU Cache
### Medium

Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.

Implement the LRUCache class:

LRUCache(int capacity) Initialize the LRU cache with positive size capacity.
int get(int key) Return the value of the key if the key exists, otherwise return -1.
void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.
The functions get and put must each run in O(1) average time complexity.

**Example:**

```
Input
["LRUCache", "put", "put", "get", "put", "get", "put", "get", "get", "get"]
[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]
Output
[null, null, null, 1, null, -1, null, -1, 3, 4]

Explanation
LRUCache lRUCache = new LRUCache(2);
lRUCache.put(1, 1); // cache is {1=1}
lRUCache.put(2, 2); // cache is {1=1, 2=2}
lRUCache.get(1);    // return 1
lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}
lRUCache.get(2);    // returns -1 (not found)
lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}
lRUCache.get(1);    // return -1 (not found)
lRUCache.get(3);    // return 3
lRUCache.get(4);    // return 4
```

**思路：**

【题外话】 面试官本职工作不是面试，本职工作已经很忙了，所以出题的时候不一定会精心设计。LRU cache，最小堆，双栈实现队列这种经典题出现的概率其实挺高的。

Get和Put必须O(1)，因此最直接的思路是哈希表。但哈希表只能记录cache里有什么，记录不了使用顺序。所以考虑使用哈希表加上双向链表来实现。哈希表记录cache里有什么，链表从头到尾记录最近使用（访问）的元素。新元素（或者访问到的旧元素）移动到链表头。

![pic](https://github.com/monocosmo/Leetcode/blob/master/Note_pics/lc146_1.PNG)

可以先分析一下使用场景，封装一些基本操作：
1. 当有新元素要加入时： add_to_head
2. 当旧元素被访问到时或者旧元素的值被更新时： remove + add_to_head
3. 当cache满，需要删除链表尾的元素时：remove_from_tail

```python
def _add_to_head(self, node: Node):
    # add one Node to the head
    node.pre = self.head
    node.next = self.head.next
    self.head.next.pre = node
    self.head.next = node

def _remove(self, node: Node):
    # remove one node
    node.pre.next = node.next
    node.next.pre = node.pre

def _remove_from_tail(self):
    # remove tail's pre node
    tail_pre = self.tail.pre
    self._remove(tail_pre)
    return tail_pre
```

最后要注意LRU cache是哈希表+双向链表实现的，在更新元素或删除尾端元素时不要忘记了哈希表和链表都要修改。

```python
# if capacity full, remove the LRU node from tail
if len(self.cache) > self.capacity:
    lru_node = self._remove_from_tail()
    del self.cache[lru_node.key]
```


**Code:**
```python
class Node:
    def __init__(self, key: int, value: int):
        self.key = key
        self.value = value
        self.pre = None
        self.next = None

class LRUCache:

    def __init__(self, capacity: int):
        # use a double linked list to store the elements
        # the most recently used element is at the head
        # the least recently used element is at the tail
        # use a cache (dictionary) to store key: node pairs
        self.capacity = capacity
        self.cache = {}
        self.head = Node(0, 0)
        self.tail = Node(0, 0)
        self.head.next = self.tail
        self.tail.pre = self.head
    
    def _add_to_head(self, node: Node):
        # add one Node to the head
        node.pre = self.head
        node.next = self.head.next
        self.head.next.pre = node
        self.head.next = node
    
    def _remove(self, node: Node):
        # remove one node
        node.pre.next = node.next
        node.next.pre = node.pre

    def _remove_from_tail(self):
        # remove tail's pre node
        tail_pre = self.tail.pre
        self._remove(tail_pre)
        return tail_pre

    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1
        
        # get the node
        node = self.cache[key]
        # remove the node from the double linked list and add it to head
        self._remove(node)
        self._add_to_head(node)
        
        return node.value

    def put(self, key: int, value: int) -> None:
        # if the key not in cache
        if key not in self.cache:
            # create a new node and add to head
            node = Node(key, value)
            self.cache[key] = node
            self._add_to_head(node)
            # if capacity full, remove the LRU node from tail
            if len(self.cache) > self.capacity:
                lru_node = self._remove_from_tail()
                del self.cache[lru_node.key]
        # if the key in cache
        else:
            # update the node value
            node = self.cache[key]
            node.value = value
            # remove the node from the double linked list and add it to head
            self._remove(node)
            self._add_to_head(node)

# Your LRUCache object will be instantiated and called as such:
# obj = LRUCache(capacity)
# param_1 = obj.get(key)
# obj.put(key,value)

# reference: https://blog.csdn.net/weixin_41012765/article/details/131639122
```
